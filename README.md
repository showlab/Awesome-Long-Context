# Awesome-Long-Context

> A curated list of resources about long-context in large-language models and video understanding.

## Contents

- [Large-Language Models](#large-language-models)
- [Long-Form Video Understanding](#long-form-video-understanding)

### Large-Language Models

- [Recurrent Memory Transformer](https://arxiv.org/abs/2207.06881)
- [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)
- [Focused transformer](https://arxiv.org/abs/2307.03170)
- [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174)
- [Memorizing Transformers](https://arxiv.org/abs/2203.08913)

### Open-source models

- [LongChat](https://github.com/lm-sys/FastChat)
- [How Long Can Open-Source LLMs Truly Promise on Context Length?](https://lmsys.org/blog/2023-06-29-longchat/)
- [LongLLaMA](https://github.com/CStanKonrad/long_llama)

### Long-Form Video Understanding

#### Papers
- [EgoVLP](https://arxiv.org/abs/2206.01670)
- [UniVTG](https://arxiv.org/abs/2307.16715)


#### Datasets

- [Ego4D](https://ego4d-data.org/)
- [Towards Long-Form Video Understanding](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Towards_Long-Form_Video_Understanding_CVPR_2021_paper.pdf)

#### Projects

- [VLog](https://github.com/showlab/VLog)
- [MovieChat](https://rese1f.github.io/MovieChat/)

#### Others

- [LOVEU workshop](https://sites.google.com/view/loveucvpr22)

## Contribute

This is a work in progress. Contributions welcome!
